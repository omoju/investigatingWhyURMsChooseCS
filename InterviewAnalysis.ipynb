{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "# (*) Pandas for data manipulation\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/InterviewDataResponses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns =  [\n",
    "'timestamp',\n",
    "'howLearnComp',\n",
    "'negAspectComp',\n",
    "'dataLab',\n",
    "'magicWand',\n",
    "'csCourseFuture',\n",
    "'currFactor',\n",
    "'changeThoughtBerkeley',\n",
    "'changePerceptnCS',\n",
    "'funAspectClass',\n",
    "'unfunAspectClass',\n",
    "'respectCourseStaff',\n",
    "'courseCulture61A',\n",
    "'changeThoughtBerkeley2',\n",
    "'moreCSClasses',\n",
    "'perceptStudyCSChange',\n",
    "'feelMinorityCS',\n",
    "'whatFirstBroughtCourseAttention',\n",
    "'reservationTakingClass',\n",
    "'probStudyCS',\n",
    "'relationshipProgrammerScientist',\n",
    "'comptThink',\n",
    "'enjoyCourse',\n",
    "'enjoyProbSolv',\n",
    "'priorCSbeforeClass',\n",
    "'academicStrenghts',\n",
    "'otherAcademicInterest',\n",
    "'majorBerkeley',\n",
    "'answerDiff',\n",
    "'strengthCS',\n",
    "'coolestAspectCS',\n",
    "'thinkSomeOneCSDo',\n",
    "'benefitStudyCS',\n",
    "'theName',\n",
    "'gender',\n",
    "'priorExposureToCS',\n",
    "'cs10',\n",
    "'cs61A',\n",
    "'race',\n",
    "'acadClass',\n",
    "'experienceSnap',\n",
    "'anythingLikeToShare',\n",
    "'extra',\n",
    "'expectGetOutOfCourse',\n",
    "'q_CT',\n",
    "'q2',\n",
    "'q3',\n",
    "'q4',\n",
    "'q5',\n",
    "'q6',\n",
    "'q7',\n",
    "'q8',\n",
    "'q9',\n",
    "'q10',\n",
    "'q11',\n",
    "'q12',\n",
    "'q13',\n",
    "'q14',\n",
    "'q15',\n",
    "'q16',\n",
    "'q17']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row, i in df.iterrows():\n",
    "    if i.race == 'white':\n",
    "        df.loc[row, 'race'] = 'WT'\n",
    "    elif i.race == 'White':\n",
    "        df.loc[row, 'race'] = 'WT'\n",
    "    elif i.race == 'black':\n",
    "        df.loc[row, 'race'] = 'BK'\n",
    "    elif i.race == 'Black':\n",
    "        df.loc[row, 'race'] = 'BK'\n",
    "    elif i.race == 'Asian':\n",
    "        df.loc[row, 'race'] = 'AS'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = df.columns.values\n",
    "for name in names:\n",
    "    print name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for theIndex, row in df[df['theName'] == 'Canzhi Ye'].iterrows():\n",
    "    print 'timestamp' , row.timestamp , '\\n'\n",
    "    print 'howLearnComp' , row.howLearnComp , '\\n'\n",
    "    print 'negAspectComp' , row.negAspectComp , '\\n'\n",
    "    print 'dataLab' , row.dataLab , '\\n'\n",
    "    print 'magicWand' , row.magicWand , '\\n'\n",
    "    print 'csCourseFuture' , row.csCourseFuture , '\\n'\n",
    "    print 'currFactor' , row.currFactor , '\\n'\n",
    "    print 'changeThoughtBerkeley' , row.changeThoughtBerkeley , '\\n'\n",
    "    print 'changePerceptnCS' , row.changePerceptnCS , '\\n'\n",
    "    print 'funAspectClass' , row.funAspectClass , '\\n'\n",
    "    print 'unfunAspectClass' , row.unfunAspectClass , '\\n'\n",
    "    print 'respectCourseStaff' , row.respectCourseStaff , '\\n'\n",
    "    print 'courseCulture61A' , row.courseCulture61A , '\\n'\n",
    "    print 'changeThoughtBerkeley2' , row.changeThoughtBerkeley2 , '\\n'\n",
    "    print 'moreCSClasses' , row.moreCSClasses , '\\n'\n",
    "    print 'perceptStudyCSChange' , row.perceptStudyCSChange , '\\n'\n",
    "    print 'feelMinorityCS' , row.feelMinorityCS , '\\n'\n",
    "    print 'whatFirstBroughtCourseAttention' , row.whatFirstBroughtCourseAttention , '\\n'\n",
    "    print 'reservationTakingClass' , row.reservationTakingClass , '\\n'\n",
    "    print 'probStudyCS' , row.probStudyCS , '\\n'\n",
    "    print 'relationshipProgrammerScientist' , row.relationshipProgrammerScientist , '\\n'\n",
    "    print 'comptThink' , row.comptThink , '\\n'\n",
    "    print 'enjoyCourse' , row.enjoyCourse , '\\n'\n",
    "    print 'enjoyProbSolv' , row.enjoyProbSolv , '\\n'\n",
    "    print 'priorCSbeforeClass' , row.priorCSbeforeClass , '\\n'\n",
    "    print 'academicStrenghts' , row.academicStrenghts , '\\n'\n",
    "    print 'otherAcademicInterest' , row.otherAcademicInterest , '\\n'\n",
    "    print 'majorBerkeley' , row.majorBerkeley , '\\n'\n",
    "    print 'answerDiff' , row.answerDiff , '\\n'\n",
    "    print 'strengthCS' , row.strengthCS , '\\n'\n",
    "    print 'coolestAspectCS' , row.coolestAspectCS , '\\n'\n",
    "    print 'thinkSomeOneCSDo' , row.thinkSomeOneCSDo , '\\n'\n",
    "    print 'benefitStudyCS' , row.benefitStudyCS , '\\n'\n",
    "    print 'theName' , row.theName , '\\n'\n",
    "    print 'gender' , row.gender , '\\n'\n",
    "    print 'priorExposureToCS' , row.priorExposureToCS , '\\n'\n",
    "    print 'cs10' , row.cs10 , '\\n'\n",
    "    print 'cs61A' , row.cs61A , '\\n'\n",
    "    print 'race' , row.race , '\\n'\n",
    "    print 'acadClass' , row.acadClass , '\\n'\n",
    "    print 'experienceSnap' , row.experienceSnap , '\\n'\n",
    "    print 'anythingLikeToShare' , row.anythingLikeToShare , '\\n'\n",
    "    print 'extra' , row.extra , '\\n'\n",
    "    print 'expectGetOutOfCourse' , row.expectGetOutOfCourse , '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('interviewText.txt', 'wt')\n",
    "try:\n",
    "    for label in df.columns:\n",
    "        #f.write(label+\"\\n\\n\")\n",
    "        #print df.columns\n",
    "        for row, columns in df.iterrows():\n",
    "            f.write(\"%s %s \\\\n \\n\" % (columns.theName, columns[label]))\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'InterviewAnalysis'\n",
    "wordlist = PlaintextCorpusReader(corpus_root, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\"\"\"\n",
    "This function takes in an object of the type PlaintextCorpusReader, and system path.\n",
    "It returns an nltk corpus\n",
    "\n",
    "It requires the regular expression package re to work\n",
    "\"\"\"\n",
    "\n",
    "def create_corpus(wordlist, some_corpus): #process the files so I know what was read in\n",
    "    for fileid in wordlist.fileids():\n",
    "        raw = wordlist.raw(fileid)\n",
    "        raw = re.split(r'\\W+', raw) ## split the raw text into appropriate words \n",
    "        ## while I am cleaning up, I should strip out the word chorus followed by the next word\n",
    "        some_corpus.extend(raw)\n",
    "        print fileid\n",
    "\n",
    "    return some_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "the_corpus = create_corpus(wordlist, []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(the_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "the_corpus[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Items = wordlist.fileids()\n",
    "Items[:14]\n",
    "[fileid for fileid in Items[:14]]\n",
    "\n",
    "print len(wordlist.words(Items[:14]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have inline graphs\n",
    "get_ipython().magic(u'matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha()) \n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words()) \n",
    "    unusual = text_vocab.difference(english_vocab)\n",
    "    return sorted(unusual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Go ahead a filter out stopwords.\n",
    "\n",
    "filtered_words = [word for word in wordlist.words() if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultUnusualWords = unusual_words(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "txt_tagged = []\n",
    "for item in Items:\n",
    "    # If i did this I wouldn't be able to see frequent phrases using common connectives\n",
    "    \n",
    "    #print unusual_words(wordlist.words(item)), '\\n\\n'\n",
    "    #filtered_words = [word for word in wordlist.words(item) if word not in stopwords.words('english')]\n",
    "    \n",
    "\n",
    "    text = ' '.join(wordlist.words(item))\n",
    "    text_tagged_sents = nltk.word_tokenize(text)\n",
    "    txt_tagged.append(nltk.pos_tag(text_tagged_sents))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(txt_tagged) # So we have 42 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in txt_tagged[:1]:\n",
    "    print sent, '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make it one string so you can use in word cloud\n",
    "text = ' '.join(resultUnusualWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    " \n",
    "wordcloud = WordCloud(background_color='white').generate(text)\n",
    "img=plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show() \n",
    " \n",
    "#or save as png\n",
    "img.write_png(\"wordcloud.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging the interview text\n",
    "\n",
    "A part-of-speech tagger, or POS tagger, processes a sequence of words, and attaches a part of speech tag to each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that searching for woman finds nouns; searching for bought mostly finds verbs; searching for over generally finds prepositions; searching for the finds several determiners. A tagger can correctly identify the tags on these words in the context of a sentence, e.g., The woman bought over $150,000 worth of clothes.\n",
    "\n",
    "Searching for three-word phrases using POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def process(sentence):\n",
    "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):\n",
    "        #print w1, w2, w3\n",
    "        if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')):\n",
    "            print w1, w2, w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tagged_sent in txt_tagged:\n",
    "    process(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect you don't just want the most common phrases, but rather you want the most interesting collocations. Otherwise, you could end up with an overrepresentation of phrases made up of common words and fewer interesting and informative phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change this to read in your data\n",
    "\n",
    "myFile = open(\"interviewText.txt\", 'r').read()\n",
    "myList = myFile.split()\n",
    "myCorpus = nltk.Text(myList)\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(myCorpus)\n",
    "\n",
    "#nltk.corpus.genesis.words('english-web.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " myCorpus.collocations(num=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.ngrams(myCorpus, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(myCorpus,window_size = 20)\n",
    "finder.apply_freq_filter(3)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in ignored_words)\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10) # doctest: +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(2) \n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "finder.nbest(trigram_measures.pmi, 75)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.node=='NP'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem_word(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        yield term\n",
    "\n",
    "        # Used when tokenizing words\n",
    "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "      ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n",
    "    | \\w+(-\\w+)*            # words with optional internal hyphens\n",
    "    | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.                # ellipsis\n",
    "    | [][.,;\"'?():-_`]      # these are separate tokens\n",
    "'''\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "#Taken from Su Nam Kim Paper...\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_phrases = {} \n",
    "text = \"\"\"\n",
    "    All my friends all of my roommates R CS majors in the seat so it's kind of interested in what they were doing, \n",
    "    I had no understanding of what they were doing so I wanted to so I decided to get some understanding for it, \n",
    "    I decided to take CS, I had a free class that I could take, I still taking it and I absolutely love it and this, \n",
    "    I decide that I wanted to put into it further.\n",
    "    \"\"\"\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "\n",
    "toks = nltk.regexp_tokenize(text, sentence_re)\n",
    "postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    #print postoks\n",
    "\n",
    "tree = chunker.parse(postoks)\n",
    "terms = get_terms(tree)\n",
    "\n",
    "for term in terms:\n",
    "    for word in term:\n",
    "        #print word,\n",
    "        if common_phrases.has_key(word):\n",
    "            common_phrases[word] += 1\n",
    "        else:\n",
    "            common_phrases[word] = 0                \n",
    "        #print\n",
    "for key in common_phrases:\n",
    "    print key,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in wordlist.sents():\n",
    "    temp = ' '.join(sent)\n",
    "    print temp, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
